{"qid": "ClassEval_56", "orig_input": "class MetricsCalculator:\n    \"\"\"\n    The class calculates precision, recall, F1 score, and accuracy based on predicted and true labels.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the number of all four samples to 0\n        \"\"\"\n        self.true_positives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.true_negatives = 0\n\n\n    def update(self, predicted_labels, true_labels):\n        \"\"\"\n        Update the number of all four samples(true_positives, false_positives, false_negatives, true_negatives)\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: None, change the number of corresponding samples\n        >>> mc = MetricsCalculator()\n        >>> mc.update([1, 1, 0, 0], [1, 0, 0, 1])\n        (self.true_positives, self.false_positives, self.false_negatives, self.true_negatives) = (1, 1, 1, 1)\n        \"\"\"\n\n\n    def precision(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate precision\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.precision([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"\n\n\n    def recall(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate recall\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.recall([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"\n\n\n    def f1_score(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate f1 score, which is the harmonic mean of precision and recall\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.f1_score([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"\n\n\n    def accuracy(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate accuracy\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>>mc.accuracy([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"", "input": "class MetricsCalculator:\n    \"\"\"\n    The class calculates precision, recall, F1 score, and accuracy based on predicted and true labels.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the number of all four samples to 0\n        \"\"\"\n        self.true_positives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.true_negatives = 0\n\n\n    def update(self, predicted_labels, true_labels):\n        \"\"\"\n        Update the number of all four samples(true_positives, false_positives, false_negatives, true_negatives)\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: None, change the number of corresponding samples\n        >>> mc = MetricsCalculator()\n        >>> mc.update([1, 1, 0, 0], [1, 0, 0, 1])\n        (self.true_positives, self.false_positives, self.false_negatives, self.true_negatives) = (1, 1, 1, 1)\n        \"\"\"\n\n\n    def precision(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate precision\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.precision([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"\n\n\n    def recall(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate recall\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.recall([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"\n\n\n    def f1_score(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate f1 score, which is the harmonic mean of precision and recall\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.f1_score([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"\n\n\n    def accuracy(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate accuracy\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>>mc.accuracy([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"", "method_info": [{"method_name": "update", "method_description": "def update(self, predicted_labels, true_labels):\n        \"\"\"\n        Update the number of all four samples(true_positives, false_positives, false_negatives, true_negatives)\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: None, change the number of corresponding samples\n        >>> mc = MetricsCalculator()\n        >>> mc.update([1, 1, 0, 0], [1, 0, 0, 1])\n        (self.true_positives, self.false_positives, self.false_negatives, self.true_negatives) = (1, 1, 1, 1)\n        \"\"\"", "test_class": "MetricsCalculatorTestUpdate", "test_code": "class MetricsCalculatorTestUpdate(unittest.TestCase):\n    def test_update_1(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 1, 1, 1))\n\n    def test_update_2(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 2, 1, 0))\n\n    def test_update_3(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (2, 1, 0, 1))\n\n    def test_update_4(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (2, 0, 1, 1))\n\n    def test_update_5(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 1, 2, 0))", "solution_code": "def update(self, predicted_labels, true_labels):\n        for predicted, true in zip(predicted_labels, true_labels):\n            if predicted == 1 and true == 1:\n                self.true_positives += 1\n            elif predicted == 1 and true == 0:\n                self.false_positives += 1\n            elif predicted == 0 and true == 1:\n                self.false_negatives += 1\n            elif predicted == 0 and true == 0:\n                self.true_negatives += 1", "dependencies": {"Standalone": false, "lib_dependencies": [], "field_dependencies": ["self.false_negatives", "self.false_positives", "self.true_negatives", "self.true_positives"], "method_dependencies": []}}, {"method_name": "precision", "method_description": "def precision(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate precision\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.precision([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"", "test_class": "MetricsCalculatorTestPrecision", "test_code": "class MetricsCalculatorTestPrecision(unittest.TestCase):\n    def test_precision_1(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_precision_2(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertAlmostEqual(temp, 0.3333333333333333)\n\n    def test_precision_3(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertAlmostEqual(temp, 0.6666666666666666)\n\n    def test_precision_4(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertAlmostEqual(temp, 1.0)\n\n    def test_precision_5(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertAlmostEqual(temp, 0.5)\n\n    def test_precision_6(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([0, 0, 0, 0], [1, 0, 1, 1])\n        self.assertAlmostEqual(temp, 0.0)", "solution_code": "def precision(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_positives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_positives)", "dependencies": {"Standalone": false, "lib_dependencies": [], "field_dependencies": ["self.false_positives", "self.true_positives"], "method_dependencies": ["update"]}}, {"method_name": "recall", "method_description": "def recall(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate recall\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.recall([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"", "test_class": "MetricsCalculatorTestRecall", "test_code": "class MetricsCalculatorTestRecall(unittest.TestCase):\n    def test_recall_1(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_recall_2(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_recall_3(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual(temp, 1.0)\n\n    def test_recall_4(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertAlmostEqual(temp, 0.6666666666666666)\n\n    def test_recall_5(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertAlmostEqual(temp, 0.3333333333333333)\n\n    def test_recall_6(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [0, 0, 0, 0])\n        self.assertEqual(temp, 0.0)", "solution_code": "def recall(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_negatives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_negatives)", "dependencies": {"Standalone": false, "lib_dependencies": [], "field_dependencies": ["self.false_negatives", "self.true_positives"], "method_dependencies": ["update"]}}, {"method_name": "f1_score", "method_description": "def f1_score(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate f1 score, which is the harmonic mean of precision and recall\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>> mc.f1_score([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"", "test_class": "MetricsCalculatorTestF1Score", "test_code": "class MetricsCalculatorTestF1Score(unittest.TestCase):\n    def test_f1_score_1(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_f1_score_2(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.4)\n\n    def test_f1_score_3(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.8)\n\n    def test_f1_score_4(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertEqual(temp, 0.8)\n\n    def test_f1_score_5(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertEqual(temp, 0.4)\n\n    def test_f1_score_6(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([0, 0, 0, 0], [0, 0, 0, 0])\n        self.assertEqual(temp, 0.0)", "solution_code": "def f1_score(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        precision = self.precision(predicted_labels, true_labels)\n        recall = self.recall(predicted_labels, true_labels)\n        if precision + recall == 0.0:\n            return 0.0\n        return (2 * precision * recall) / (precision + recall)", "dependencies": {"Standalone": false, "lib_dependencies": [], "field_dependencies": [], "method_dependencies": ["update", "precision", "recall"]}}, {"method_name": "accuracy", "method_description": "def accuracy(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculate accuracy\n        :param predicted_labels: list, predicted results\n        :param true_labels: list, true labels\n        :return: float\n        >>> mc = MetricsCalculator()\n        >>>mc.accuracy([1, 1, 0, 0], [1, 0, 0, 1])\n        0.5\n        \"\"\"", "test_class": "MetricsCalculatorTestAccuracy", "test_code": "class MetricsCalculatorTestAccuracy(unittest.TestCase):\n    def test_accuracy_1(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_accuracy_2(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 2, 0], [1, 0, 0, 1])\n        self.assertAlmostEqual(temp, 0.3333333333333333)\n\n    def test_accuracy_3(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.75)\n\n    def test_accuracy_4(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertEqual(temp, 0.75)\n\n    def test_accuracy_5(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertEqual(temp, 0.25)\n\n    def test_accuracy_6(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([], [])\n        self.assertEqual(temp, 0.0)", "solution_code": "def accuracy(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\n        if total == 0:\n            return 0.0\n        return (self.true_positives + self.true_negatives) / total", "dependencies": {"Standalone": false, "lib_dependencies": [], "field_dependencies": ["self.false_negatives", "self.false_positives", "self.true_negatives", "self.true_positives"], "method_dependencies": ["update"]}}], "test_cases": "import unittest\n\n\nclass MetricsCalculatorTestUpdate(unittest.TestCase):\n    def test_update_1(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 1, 1, 1))\n\n    def test_update_2(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 2, 1, 0))\n\n    def test_update_3(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (2, 1, 0, 1))\n\n    def test_update_4(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (2, 0, 1, 1))\n\n    def test_update_5(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 1, 2, 0))\n\n\nclass MetricsCalculatorTestPrecision(unittest.TestCase):\n    def test_precision_1(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_precision_2(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertAlmostEqual(temp, 0.3333333333333333)\n\n    def test_precision_3(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertAlmostEqual(temp, 0.6666666666666666)\n\n    def test_precision_4(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertAlmostEqual(temp, 1.0)\n\n    def test_precision_5(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertAlmostEqual(temp, 0.5)\n\n    def test_precision_6(self):\n        mc = MetricsCalculator()\n        temp = mc.precision([0, 0, 0, 0], [1, 0, 1, 1])\n        self.assertAlmostEqual(temp, 0.0)\n\n\nclass MetricsCalculatorTestRecall(unittest.TestCase):\n    def test_recall_1(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_recall_2(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_recall_3(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual(temp, 1.0)\n\n    def test_recall_4(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertAlmostEqual(temp, 0.6666666666666666)\n\n    def test_recall_5(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertAlmostEqual(temp, 0.3333333333333333)\n\n    def test_recall_6(self):\n        mc = MetricsCalculator()\n        temp = mc.recall([1, 1, 0, 0], [0, 0, 0, 0])\n        self.assertEqual(temp, 0.0)\n\n\nclass MetricsCalculatorTestF1Score(unittest.TestCase):\n    def test_f1_score_1(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_f1_score_2(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 1, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.4)\n\n    def test_f1_score_3(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.8)\n\n    def test_f1_score_4(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertEqual(temp, 0.8)\n\n    def test_f1_score_5(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertEqual(temp, 0.4)\n\n    def test_f1_score_6(self):\n        mc = MetricsCalculator()\n        temp = mc.f1_score([0, 0, 0, 0], [0, 0, 0, 0])\n        self.assertEqual(temp, 0.0)\n\n\nclass MetricsCalculatorTestAccuracy(unittest.TestCase):\n    def test_accuracy_1(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n\n    def test_accuracy_2(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 2, 0], [1, 0, 0, 1])\n        self.assertAlmostEqual(temp, 0.3333333333333333)\n\n    def test_accuracy_3(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 1], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.75)\n\n    def test_accuracy_4(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 0], [1, 1, 0, 1])\n        self.assertEqual(temp, 0.75)\n\n    def test_accuracy_5(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([1, 1, 0, 0], [1, 0, 1, 1])\n        self.assertEqual(temp, 0.25)\n\n    def test_accuracy_6(self):\n        mc = MetricsCalculator()\n        temp = mc.accuracy([], [])\n        self.assertEqual(temp, 0.0)\n\n\nclass MetricsCalculatorTest(unittest.TestCase):\n    def test_metricscalculator(self):\n        mc = MetricsCalculator()\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (0, 0, 0, 0))\n        mc.update([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual((mc.true_positives, mc.false_positives, mc.false_negatives, mc.true_negatives), (1, 1, 1, 1))\n        temp = mc.precision([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n        temp = mc.recall([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n        temp = mc.f1_score([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)\n        temp = mc.accuracy([1, 1, 0, 0], [1, 0, 0, 1])\n        self.assertEqual(temp, 0.5)", "solutions": ["class MetricsCalculator:\n    def __init__(self):\n        self.true_positives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.true_negatives = 0\n\n    def update(self, predicted_labels, true_labels):\n        for predicted, true in zip(predicted_labels, true_labels):\n            if predicted == 1 and true == 1:\n                self.true_positives += 1\n            elif predicted == 1 and true == 0:\n                self.false_positives += 1\n            elif predicted == 0 and true == 1:\n                self.false_negatives += 1\n            elif predicted == 0 and true == 0:\n                self.true_negatives += 1\n\n    def precision(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_positives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_positives)\n\n    def recall(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_negatives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_negatives)\n\n    def f1_score(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        precision = self.precision(predicted_labels, true_labels)\n        recall = self.recall(predicted_labels, true_labels)\n        if precision + recall == 0.0:\n            return 0.0\n        return (2 * precision * recall) / (precision + recall)\n\n    def accuracy(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\n        if total == 0:\n            return 0.0\n        return (self.true_positives + self.true_negatives) / total"], "task": "class_eval", "split": "test", "transformation_type": "vanilla", "import_statement": [], "test_classes": ["MetricsCalculatorTestUpdate", "MetricsCalculatorTestPrecision", "MetricsCalculatorTestRecall", "MetricsCalculatorTestF1Score", "MetricsCalculatorTestAccuracy", "MetricsCalculatorTest"]}