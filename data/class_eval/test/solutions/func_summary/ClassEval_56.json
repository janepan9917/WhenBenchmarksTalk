{"qid": "ClassEval_56", "split": "test", "solutions": ["class MetricsCalculator:\n    def __init__(self):\n        self.true_positives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.true_negatives = 0\n\n    def update(self, predicted_labels, true_labels):\n        for predicted, true in zip(predicted_labels, true_labels):\n            if predicted == 1 and true == 1:\n                self.true_positives += 1\n            elif predicted == 1 and true == 0:\n                self.false_positives += 1\n            elif predicted == 0 and true == 1:\n                self.false_negatives += 1\n            elif predicted == 0 and true == 0:\n                self.true_negatives += 1\n\n    def precision(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_positives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_positives)\n\n    def recall(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_negatives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_negatives)\n\n    def f1_score(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        precision = self.precision(predicted_labels, true_labels)\n        recall = self.recall(predicted_labels, true_labels)\n        if precision + recall == 0.0:\n            return 0.0\n        return (2 * precision * recall) / (precision + recall)\n\n    def accuracy(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\n        if total == 0:\n            return 0.0\n        return (self.true_positives + self.true_negatives) / total"]}